Published: 2018-04-27
Title: ML in practice, part 2
Lead: Continuing from the last time, let's solve some actual problems
Author: Bartosz
Tags:
  - Machine Learning
  - Coursera
---

[Last time](/posts/Learning-ML-from-theory-to-practice) I just set up the environment and did a quick test on a simple, generated data. Fortunately for me ```scikit-learn``` comes with a few build-in datasets that you can play with. I could probably fit some Neural Network quickly and be done with it, but, as I want to practice what I've learned on the course, I will try to solve something with Linear Regression first.

### Quick try

I looked at the datasets present in the library and decided that [Boston hource pricing](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston) might be a good place to start. 

Instead of mulling over the data, let's try to quickly fit a simple linear regression and see how it goes:

```
import sklearn.datasets as ds
import sklearn.model_selection as md

data = ds.load_boston();

X_train, X_test, y_train, y_test = md.train_test_split(
     data.data, data.target)

import sklearn.linear_model as lm

reg = lm.LinearRegression();

reg.fit(X_train, y_train)

print(reg.score(X_train, y_train))
print(reg.score(X_test, y_test))
```

```
Train score: 0.7502735715415614
Test score: 0.6982613702509022
```

Hm... that's not really greata  score. Let's see if we can make it better.

### Learning curves

One of the way to 'debug' the algorithm that I learned in the course was plotting learning curves. Durng the course, I had to save J (the error) during each step of the fitting. Fortunately, ```scikit``` comes with a neat function that does everything for you - [learning_curve](http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html)

```
from sklearn.model_selection import learning_curve
from matplotlib import pyplot

train_sizes, train_scores, cv_scores = learning_curve(reg, X_train, y_train)

train_scores_mean = np.mean(train_scores, axis=1)
cv_scores_mean = np.mean(cv_scores, axis=1)

pyplot.ylim((0.3, 1.01))

pyplot.plot(train_sizes, train_scores_mean, 'ro-', label="Train score")
pyplot.plot(train_sizes, cv_scores_mean, 'go-', label="CV score")
pyplot.legend()
```

![Learning curves](/content/posts/learning-ml-part-2/learning-curves-1.png "Learning curves")

This plot tells me two things:
1. Train and CV score converge so it's not a high variance problem
2. The score of both is pretty low, so it might be a high bias problem

High bias problem usually stems from model not fitting the data well. Let's look at the data